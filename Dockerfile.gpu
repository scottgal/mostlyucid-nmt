# Dockerfile.gpu â€” GPU-enabled image using CUDA runtime with preloaded models
# Build example:
#   docker build -f Dockerfile.gpu -t scottgal/mostlylucid-nmt:gpu --build-arg VERSION=$(date -u +"%Y%m%d.%H%M%S") .
# Run example (requires NVIDIA Container Toolkit):
#   docker run --gpus all -e USE_GPU=true -e PRELOAD_MODELS="en->de,de->en" -p 8000:8000 scottgal/mostlylucid-nmt:gpu

FROM nvidia/cuda:12.6.2-cudnn-runtime-ubuntu24.04

# Build arguments for versioning
ARG VERSION=dev
ARG BUILD_DATE
ARG VCS_REF

# OCI labels for metadata
LABEL org.opencontainers.image.title="mostlylucid-nmt" \
      org.opencontainers.image.description="FastAPI neural machine translation service - GPU with CUDA 12.6 and preloaded models" \
      org.opencontainers.image.version="${VERSION}" \
      org.opencontainers.image.created="${BUILD_DATE}" \
      org.opencontainers.image.source="https://github.com/scottgal/mostlylucid-nmt" \
      org.opencontainers.image.revision="${VCS_REF}" \
      org.opencontainers.image.vendor="scottgal" \
      org.opencontainers.image.licenses="MIT" \
      org.opencontainers.image.documentation="https://github.com/scottgal/mostlylucid-nmt/blob/main/README.md" \
      variant="gpu-full"

ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PIP_NO_CACHE_DIR=1

# System deps
RUN apt-get update && apt-get install -y --no-install-recommends \
        python3 \
        python3-pip \
        python3-venv \
        build-essential \
        libssl-dev \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

COPY requirements.txt ./

# Install Python deps from requirements first (will install CPU torch, then we upgrade to CUDA wheel)
# Ubuntu 24.04 requires --break-system-packages for pip installs outside venv
RUN pip3 install --break-system-packages -r requirements.txt

# Install CUDA-enabled PyTorch matching CUDA 12.4 (compatible with CUDA 12.6 runtime)
# See https://pytorch.org for the latest index URL mapping
RUN pip3 install --break-system-packages --upgrade --index-url https://download.pytorch.org/whl/cu124 torch

COPY src/ ./src/
COPY public/ ./public/
COPY app.py ./

EXPOSE 8000

# Common runtime envs:
# - USE_GPU=true|false|auto (default auto)
# - DEVICE=auto|cpu|cuda|cuda:0 (overrides USE_GPU when set)
# - PRELOAD_MODELS="en->de,de->en,fr->en" (preload at startup)
# - MAX_CACHED_MODELS=6 (LRU capacity)

# Ensure fast, graceful shutdown inside the container
STOPSIGNAL SIGTERM

# Exec so Gunicorn is PID 1 and receives signals directly; set shorter graceful timeouts
CMD ["bash", "-lc", "exec gunicorn -k uvicorn.workers.UvicornWorker -w ${WEB_CONCURRENCY:-1} -b 0.0.0.0:8000 --timeout ${TIMEOUT:-60} --graceful-timeout ${GRACEFUL_TIMEOUT:-20} --keepalive ${KEEP_ALIVE:-5} app:app"]
