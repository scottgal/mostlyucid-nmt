# Dockerfile.intel.min â€” Minimal Intel GPU/NPU-enabled image with no preloaded models
#
# Supports Intel Arc GPUs, Data Center GPUs, integrated GPUs, and potentially NPUs
# Uses Intel Extension for PyTorch (IPEX) for hardware acceleration
#
# FAST BUILD (no model downloads - ~30 seconds):
#   docker build -f Dockerfile.intel.min -t scottgal/mostlylucid-nmt:intel-min --build-arg VERSION=$(date -u +"%Y%m%d.%H%M%S") .
#
# Run example (requires Intel GPU):
#   docker run --device=/dev/dri -e USE_GPU=true -v ./model-cache:/models -p 8000:8000 scottgal/mostlylucid-nmt:intel-min
#
# FASTEST DEVELOPMENT WORKFLOW (code updates only):
#   1. Build once: docker build -f Dockerfile.intel.min -t dev-intel .
#   2. Code-only rebuild: ~3 seconds (just copies updated .py files)
#   3. Zero-rebuild option: Mount source code directly
#
#      docker run --device=/dev/dri -p 8000:8000 -v $(pwd)/src:/app/src -v $(pwd)/app.py:/app/app.py -v ./model-cache:/models dev-intel
#
# LAYER CACHING: Optimised for fast rebuilds
#   1. Base image (cached)
#   2. System packages (cached)
#   3. Python dependencies (cached unless requirements.txt changes)
#   4. Source code (rebuilds on .py changes - FAST, ~3 seconds!)
#
FROM python:3.12-slim

# Build arguments for versioning
ARG VERSION=dev
ARG BUILD_DATE
ARG VCS_REF

# OCI labels for metadata
LABEL org.opencontainers.image.title="mostlylucid-nmt" \
      org.opencontainers.image.description="FastAPI neural machine translation service - Intel GPU/NPU, no preloaded models" \
      org.opencontainers.image.version="${VERSION}" \
      org.opencontainers.image.created="${BUILD_DATE}" \
      org.opencontainers.image.source="https://github.com/scottgal/mostlylucid-nmt" \
      org.opencontainers.image.revision="${VCS_REF}" \
      org.opencontainers.image.vendor="scottgal" \
      org.opencontainers.image.licenses="MIT" \
      org.opencontainers.image.documentation="https://github.com/scottgal/mostlylucid-nmt/blob/main/README.md" \
      variant="intel-min"

ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PIP_NO_CACHE_DIR=1 \
    # Point all caches to the mounted volume to avoid baking anything into the image
    HF_HOME=/models \
    HF_DATASETS_CACHE=/models \
    TORCH_HOME=/models \
    # Performance defaults optimised for Intel GPU (fast as possible)
    # MAX_INFLIGHT_TRANSLATIONS should equal number of GPUs (1 GPU = 1 inflight)
    # Concurrency is handled by EASYNMT_BATCH_SIZE (64) batching multiple texts within each request
    WEB_CONCURRENCY=1 \
    MAX_INFLIGHT_TRANSLATIONS=1 \
    EASYNMT_BATCH_SIZE=64 \
    EASYNMT_MODEL_ARGS='{"torch_dtype":"fp16"}' \
    MAX_CACHED_MODELS=10 \
    ENABLE_QUEUE=1 \
    MAX_QUEUE_SIZE=2000 \
    # Fast shutdown (5s graceful timeout instead of 20s)
    GRACEFUL_TIMEOUT=5

# System deps (Python + GPU libraries)
RUN apt-get update && apt-get install -y --no-install-recommends \
        ca-certificates \
        git \
        wget \
        gnupg2 \
        # Intel GPU runtime dependencies
        intel-opencl-icd \
        intel-level-zero-gpu \
        level-zero \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

COPY requirements-prod.txt ./

# Install dependencies excluding torch (we'll install it separately with Intel extension)
RUN grep -v "^torch\>" requirements-prod.txt > requirements.notorch.txt && \
    pip install --no-cache-dir -r requirements.notorch.txt

# Install PyTorch with Intel Extension for PyTorch (IPEX)
# This provides Intel GPU and NPU support
RUN pip install --no-cache-dir \
    torch==2.3.1+cxx11.abi \
    intel-extension-for-pytorch==2.3.110+xpu \
    oneccl_bind_pt==2.3.100+xpu \
    --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/

# Copy source code
COPY src/ ./src/
COPY public/ ./public/
COPY app.py ./

# Create directory for model cache (can be mounted as volume)
RUN mkdir -p /models

# Set default model cache directory
ENV MODEL_CACHE_DIR=/models

# Don't preload any models by default (override with PRELOAD_MODELS env var)
ENV PRELOAD_MODELS=""

# Common runtime envs:
# - USE_GPU=true|false|auto (default auto - Intel GPUs use XPU backend)
# - DEVICE=auto|cpu|xpu|xpu:0 (Intel GPUs use 'xpu' device)
# - PRELOAD_MODELS="en->de,de->en,fr->en" (preload at startup)
# - MAX_CACHED_MODELS=6 (LRU capacity)
# - MODEL_FAMILY=opus-mt|mbart50|m2m100 (default opus-mt)

EXPOSE 8000

# Ensure fast, graceful shutdown inside the container
STOPSIGNAL SIGTERM

# Exec so Gunicorn is PID 1 and receives signals directly; set shorter graceful timeouts
CMD ["bash", "-lc", "exec gunicorn -k uvicorn.workers.UvicornWorker -w ${WEB_CONCURRENCY:-1} -b 0.0.0.0:8000 --timeout ${TIMEOUT:-60} --graceful-timeout ${GRACEFUL_TIMEOUT:-20} --keep-alive ${KEEP_ALIVE:-5} app:app"]
